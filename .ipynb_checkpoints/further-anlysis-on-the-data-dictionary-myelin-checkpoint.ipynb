{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction / Basic Information\n",
    "\n",
    "### Here I am going to visualise, get familiar, clean and analyse the final data collected and the dictionary with the epochs created by Ilia and Aman in Myelin's Hacker House/ June 2023.\n",
    "\n",
    "\n",
    "## About the data acquisition \n",
    "\n",
    "![](https://media.licdn.com/dms/image/C560BAQHAvDDmK6yunw/company-logo_200_200/0/1519884058340?e=2147483647&v=beta&t=422BH8vOzT7W8TKoXfvMXEQcdY8TNISZ3w2Qy4uOKXs)\n",
    "\n",
    "For the eeg recordings we used Mentalab's headset Explore+ \n",
    "\n",
    "**Sample Freq:** 256 Hz\n",
    "\n",
    "**Channels Used:**  C3, CP3, P3, P4, C4, F4, CP4, F5\n",
    "\n",
    "\n",
    "\n",
    "![](https://media.discordapp.net/attachments/1107723057675645121/1115625454406213652/IMG_20230603_153825.jpg?width=862&height=645)\n",
    "\n",
    "## About the method to get the emotions in time accuracy \n",
    "For the binary classficiation of the emotional state to train a model we used flappy bird as described more Myelin's Hacker House blog here: https://myelin.substack.com/p/fuzzy-logic-and-flappy-bird?sd=pf\n",
    "\n",
    "![](https://images-ext-1.discordapp.net/external/6LliJIqDr2WTN6LI2LCsgsJsIhACDfKRnMYxPHufpTU/https/codesandbox.io/api/v1/sandboxes/h2h00z/screenshot.png)\n",
    "\n",
    "## About the baseline binary group of events / epochs selection and form of dictionary - data structure - \n",
    "\n",
    "### Disclaimer\n",
    "> **We have decided to test the data and the classification algorithms on a extremely easy set ...\n",
    ">  the data is not enough / not 100% clean proffesionally collected and the 2 states of classification not ideal but we are focusing on that more for creating the important pipeline which will be the same for any other eeg emotion state classification and open source. ****\n",
    "\n",
    "### Dictionary of epochs stucture\n",
    "\n",
    "\n",
    "**Duration of epochs:** 4 seconds  ( that can be changed )\n",
    "\n",
    "**Labels** Two states out of the game:\n",
    "1) **neutral**: any event after 2 and before 2 seconds from start and end of game session/ focused without feelings\n",
    "2) **end**:  2 seconds before losing and 2 seconds after restarting the new game session. anticipation/ sadness/ stress/ dissapointment\n",
    "\n",
    "![](https://media.discordapp.net/attachments/1102775621991530510/1117173936887697518/image.png?width=915&height=645)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the code the guy's wrote for creating the dictionary containg the data we want...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:15.647810Z",
     "iopub.status.busy": "2023-06-13T15:43:15.646985Z",
     "iopub.status.idle": "2023-06-13T15:43:15.651823Z",
     "shell.execute_reply": "2023-06-13T15:43:15.651140Z",
     "shell.execute_reply.started": "2023-06-13T15:43:15.647776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:15.691375Z",
     "iopub.status.busy": "2023-06-13T15:43:15.691002Z",
     "iopub.status.idle": "2023-06-13T15:43:15.709015Z",
     "shell.execute_reply": "2023-06-13T15:43:15.707905Z",
     "shell.execute_reply.started": "2023-06-13T15:43:15.691348Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataAnalysis():\n",
    "    def __init__(self, path_EEG, path_FlappyBird, window_size_sec, UNIX, LSL):\n",
    "        self.df_E = pd.read_csv(path_EEG)\n",
    "        self.df_F = pd.read_csv(path_FlappyBird)\n",
    "        self.channels = pd.read_csv(path_EEG).columns[1:]\n",
    "        print(self.channels)\n",
    "        self.window = window_size_sec\n",
    "        self.offset = UNIX - LSL\n",
    "        self.df_E = self.df_E.rename(columns={\"TimeStamp\": \"Timestamp\"})\n",
    "        self.df_E['Timestamp'] = (self.df_E['Timestamp']*1000).astype('int') + self.offset\n",
    "    \n",
    "    def find_nearest(self, array, value):\n",
    "        array = np.asarray(array)\n",
    "        value_true = array[(np.abs(array - value)).argmin()]\n",
    "        return value_true\n",
    "        \n",
    "    def dataframe_of_events_and_merge(self):\n",
    "        #This function gets the timestamps of all events Start and Over \n",
    "        df = self.df_E.merge(self.df_F, on='Timestamp', how='outer')\n",
    "        df = df.sort_values(by=\"Timestamp\").reset_index()\n",
    "        event_rows = df[(df.Event == 'Start') | (df.Event == 'Over')].index\n",
    "        return df.iloc[event_rows], df\n",
    "    \n",
    "    def get_window(self, timestamp):\n",
    "        window_df = pd.DataFrame()\n",
    "        lower = timestamp - self.window * 1000\n",
    "        upper = timestamp + self.window * 1000\n",
    "        window_df = self.df_E[(self.df_E.Timestamp >= lower) & (self.df_E.Timestamp <= upper)]\n",
    "        return window_df\n",
    "    \n",
    "    def plot_window(self, event_number):\n",
    "        events_df, df = self.dataframe_of_events_and_merge()\n",
    "        timestamp = events_df.iloc[event_number].Timestamp\n",
    "        window_df = self.get_window(timestamp)\n",
    "        for channel in self.channels:\n",
    "            window_array = window_df[channel]\n",
    "            plt.plot(window_df.Timestamp, window_array, color='red')\n",
    "            plt.title(\"Node:\" + channel)\n",
    "            plt.plot(np.ones(1000)*events_df.iloc[event_number]['Timestamp'], np.linspace(min(window_array), max(window_array), 1000))\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    def get_analysis_timestamps(self):\n",
    "        events_df, df = self.dataframe_of_events_and_merge()\n",
    "        window_adj = self.window*2\n",
    "        dictionary = {'neutral':[], 'end':[]}\n",
    "        events = events_df['Timestamp'].to_numpy()\n",
    "        # Getting the timestamps of events we need\n",
    "        for i in range(0, len(events_df), 2):\n",
    "            time = (events[i+1] - events[i])/1000\n",
    "            if time > window_adj:\n",
    "                dictionary['end'].append(events[i+1])\n",
    "            if time > window_adj*2 and time < 45:\n",
    "                counter = 1\n",
    "                while events[i] + window_adj*counter*1000 < events[i+1]:\n",
    "                    dictionary['neutral'].append(events[i] + window_adj*counter*1000)\n",
    "                    counter+=1\n",
    "                    \n",
    "        #If there's no timestamp corresponding to neutral one exactly, we find the nearest one\n",
    "        true_array = []\n",
    "        for element in dictionary['neutral']:\n",
    "            true_array.append(self.find_nearest(self.df_E['Timestamp'].to_numpy(), element))\n",
    "        dictionary['neutral'] = true_array\n",
    "#         dictionary['end'] = list(dictionary['end'])\n",
    "        #we replace each element of dictionary with 2-d array of size 9*n\n",
    "        for i in range(len(dictionary['end'])):\n",
    "            dictionary['end'][i] = self.get_window(dictionary['end'][i]).to_numpy()\n",
    "        for j in range(len(dictionary['neutral'])):\n",
    "            dictionary['neutral'][j] = self.get_window(dictionary['neutral'][j]).to_numpy()\n",
    "        return dictionary        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that is uploaded on Myelin's Github Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:15.724309Z",
     "iopub.status.busy": "2023-06-13T15:43:15.723949Z",
     "iopub.status.idle": "2023-06-13T15:43:15.733448Z",
     "shell.execute_reply": "2023-06-13T15:43:15.732304Z",
     "shell.execute_reply.started": "2023-06-13T15:43:15.724280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sourcing our recorded data, df_f = flappy bird trials, df_e = eeg recordings\n",
    "df_f1 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_1/FlappyB_1.csv\"\n",
    "df_f2 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_2/FlappyB_2.csv\"\n",
    "df_f3 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_3/FlappyB_3.csv\"\n",
    "df_f4 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_4/FlappyB_4.csv\"\n",
    "df_f5 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_5/FlappyB_5.csv\"\n",
    "df_f6 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_6/FlappyB_6.csv\"\n",
    "df_f7 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_7/FlappyB_7.csv\"\n",
    "df_f8 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_8/FlappyB_8.csv\"\n",
    "df_f9 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_9/FlappyB_9.csv\"\n",
    "df_f10 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_10/FlappyB_10.csv\"\n",
    "df_f11 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_11/FlappyB_11.csv\"\n",
    "df_f12 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_12/FlappyB_12.csv\"\n",
    "df_f13 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_13/FlappyB_13.csv\"\n",
    "df_f14 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_14/FlappyB_14.csv\"\n",
    "df_f15 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_15/FlappyB_15.csv\"\n",
    "\n",
    "\n",
    "df_e1 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_1/FlappyBird_Trial_1_ExG.csv\"\n",
    "df_e2 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_2/FlappyBird_Trial_2_ExG.csv\"\n",
    "df_e3 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_3/FlappyBird_Trial_3_ExG.csv\"\n",
    "df_e4 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_4/FlappyBird_Trial_4_ExG.csv\"\n",
    "df_e5 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_5/FlappyBird_Trial_5_ExG.csv\"\n",
    "df_e6 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_6/FlappyBird_Trial_6_ExG.csv\"\n",
    "df_e7 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_7/FlappyBird_Trial_7_ExG.csv\"\n",
    "df_e8 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_8/FlappyBird_Trial_8_ExG.csv\"\n",
    "df_e9 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_9/FlappyBird_Trial_9_ExG.csv\"\n",
    "df_e10 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_10/FlappyBird_Trial_10_ExG.csv\"\n",
    "df_e11 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_11/FlappyBird_Trial_11_ExG.csv\"\n",
    "df_e12 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_12/FlappyBird_Trial_12_ExG.csv\"\n",
    "df_e13 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_13/FlappyBird_Trial_13_ExG.csv\"\n",
    "df_e14 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_14/FlappyBird_Trial_14_ExG.csv\"\n",
    "df_e15 = \"https://raw.githubusercontent.com/MyelinGroup/myelin-fuzzy-logic/main/Our%20Recorded%20Data/FlappyBird_Trial_15/FlappyBird_Trial_15_ExG.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary based on recording number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:15.787117Z",
     "iopub.status.busy": "2023-06-13T15:43:15.786716Z",
     "iopub.status.idle": "2023-06-13T15:43:19.912534Z",
     "shell.execute_reply": "2023-06-13T15:43:19.911443Z",
     "shell.execute_reply.started": "2023-06-13T15:43:15.787073Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# timestmaps for rec 5 (1686490572615, 2453683) got it from timestamps_trial_5.txt file github of specifci recoridng folder\n",
    "# timestamps for rec 4 (1686490104157, 1985220)    >> >>  >>         >>\n",
    "\n",
    "# timestamps for rec 2 ((1686188325437, 943743)    >> >>  >>         >>  1686188325437, 943743\n",
    "\n",
    "             # Recording 2 might be the best one so far so were gonna work on that\n",
    "    \n",
    "time_window= 2       # 2 sec before + 2 seconds after = 4 seconds total \n",
    "\n",
    "model = DataAnalysis(df_e2, df_f2, time_window, 1686188325437, 943743) \n",
    "\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.plot_window(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:19.915274Z",
     "iopub.status.busy": "2023-06-13T15:43:19.914687Z",
     "iopub.status.idle": "2023-06-13T15:43:20.167225Z",
     "shell.execute_reply": "2023-06-13T15:43:20.166216Z",
     "shell.execute_reply.started": "2023-06-13T15:43:19.915244Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# UNDERSTAND THE DATA\n",
    "\n",
    "neutral=model.get_analysis_timestamps()['neutral'] #,dtype=object)\n",
    "end=model.get_analysis_timestamps()['end']\n",
    "\n",
    "\n",
    "print(len(neutral))\n",
    "\n",
    "print(len(end))\n",
    "\n",
    "\n",
    "one_event = neutral[11]\n",
    "\n",
    "print(one_event.shape)\n",
    "print(one_event)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:20.168866Z",
     "iopub.status.busy": "2023-06-13T15:43:20.168559Z",
     "iopub.status.idle": "2023-06-13T15:43:20.181541Z",
     "shell.execute_reply": "2023-06-13T15:43:20.180614Z",
     "shell.execute_reply.started": "2023-06-13T15:43:20.168841Z"
    }
   },
   "outputs": [],
   "source": [
    "###              Let's make it more readable to the human eye so it makes sense\n",
    "\n",
    "df = pd.DataFrame(one_event)\n",
    "df = pd.DataFrame(one_event, columns=['Timestamps','C3', 'CP3','P3','P4',' C4', 'F4', 'CP4','F5'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:20.184470Z",
     "iopub.status.busy": "2023-06-13T15:43:20.184169Z",
     "iopub.status.idle": "2023-06-13T15:43:20.470872Z",
     "shell.execute_reply": "2023-06-13T15:43:20.469852Z",
     "shell.execute_reply.started": "2023-06-13T15:43:20.184445Z"
    }
   },
   "outputs": [],
   "source": [
    "df.plot(x='Timestamps', y='C3')  #, kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok seems correct with the plot windows so now we are confident of where the data is and we can move on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose the optimal channel for the classification...\n",
    "the above code filters in a ctuoff of freq= 60 hz but doesnt handle other artifacts such as muscle moveents, eye blinks, electrode dysplacement etc.\n",
    "We can visualise for example bellow channels , to understnad the difference between a good and a bad signal recording.\n",
    "![](https://www.intechopen.com/media/chapter/54606/media/F2.png)\n",
    "\n",
    "\n",
    "Cleaning an EEG signal is complex but we can minimise the noise..\n",
    "methods used could be:\n",
    "\n",
    "- filtering on certian freq (already done above) \n",
    "- threshold for spefific amplitude muscle movements\n",
    "- wavelets when you know the shape of the noise you expect or NN on them for example eye blinks\n",
    "- Independent Component Analysis (ICA) method used to separate mixed signals into their underlying independent component\n",
    "- Other metrics such as standad deviation to see how far are distributed the points of the signal. Remember though that in eeg we expect them to not be so close3 to each other as they are time series biosignals, and also a small std could mean that it is puely noise..so we have to take care of a threshold in that case.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> ****One more thing we should point out is that we want to create a BCI app for well-being and fun. So we care about the analogy of the signal and not the actual form that is crucial for medical devices and clinical diagnosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the above event that we plotted let's chech ICA and std for cleaning/ smoothing\n",
    "\n",
    "### STD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:20.473439Z",
     "iopub.status.busy": "2023-06-13T15:43:20.472519Z",
     "iopub.status.idle": "2023-06-13T15:43:20.510860Z",
     "shell.execute_reply": "2023-06-13T15:43:20.509821Z",
     "shell.execute_reply.started": "2023-06-13T15:43:20.473399Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw on the window plot C3, CP3, F4 and F5 looked more clean and we can also understand it better as we see above that the std is lower that the noisy channels.\n",
    "One easy thing would be to select the channel with the minimum std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:20.512866Z",
     "iopub.status.busy": "2023-06-13T15:43:20.512379Z",
     "iopub.status.idle": "2023-06-13T15:43:20.558261Z",
     "shell.execute_reply": "2023-06-13T15:43:20.557207Z",
     "shell.execute_reply.started": "2023-06-13T15:43:20.512827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Better apply that to normalised data between 0-1\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "  \n",
    "scaler = MinMaxScaler()      # 0-1\n",
    "#scaler = StandardScaler()    # zscored \n",
    "\n",
    "normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# check the normalized event\n",
    "print(normalized_df)\n",
    "normalized_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA\n",
    "\n",
    "in case you get confused sometimes as I do about all the component analysis methods:\n",
    "> To summup: PCA aims to create new components that hold the maximum variance of the input. LDA aims to create new components that separate clusters based on a categorical feature. ICA aims to retrieve original features that are mixed together in a linear combination in the input dataset.\n",
    "\n",
    "more here: https://towardsdatascience.com/pca-lda-ica-a-components-analysis-algorithms-comparison-c5762c4148ff#:~:text=To%20summup%3A,combination%20in%20the%20input%20dataset\n",
    "\n",
    "\n",
    "![](https://team.inria.fr/parietal/files/2018/07/ica_principle.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:20.560051Z",
     "iopub.status.busy": "2023-06-13T15:43:20.559456Z",
     "iopub.status.idle": "2023-06-13T15:43:20.846330Z",
     "shell.execute_reply": "2023-06-13T15:43:20.845491Z",
     "shell.execute_reply.started": "2023-06-13T15:43:20.560025Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "channel='C3'\n",
    "\n",
    "print(normalized_df[[channel]])\n",
    "ica = FastICA(n_components=1)\n",
    "ica_result = ica.fit_transform(normalized_df[[channel]])\n",
    "\n",
    "ica_df = pd.DataFrame(ica_result, columns=['ICA'])\n",
    "\n",
    "plt.figure()\n",
    "normalized_df.plot(y=channel)\n",
    "\n",
    "column_data = normalized_df[channel].values.reshape(-1, 1)\n",
    "\n",
    "plt.plot(ica_df['ICA'])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('ICA Component')\n",
    "plt.title('ICA Component Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  it is expected that the main change would be **the amplitude reduction**. ICA aims to separate mixed signals into statistically independent components, but it **does not necessarily alter the morphology** or shape of the original signal significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet \n",
    "\n",
    "The Wavelet Transform decomposes a signal into different frequency bands and provides information about the temporal localization of each frequency component. By analyzing the wavelet coefficients, you can gain insights into the signal's frequency content and how it varies over time.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Continuous_wavelet_transform.gif/300px-Continuous_wavelet_transform.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:20.848248Z",
     "iopub.status.busy": "2023-06-13T15:43:20.847364Z",
     "iopub.status.idle": "2023-06-13T15:43:21.799822Z",
     "shell.execute_reply": "2023-06-13T15:43:21.799110Z",
     "shell.execute_reply.started": "2023-06-13T15:43:20.848218Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=normalized_df\n",
    "channel='C3'\n",
    "# Apply Wavelet Transform\n",
    "wavelet = 'db4'  # Wavelet type (choose according to your requirements)\n",
    "level = 5  # Number of decomposition levels\n",
    "coeffs = pywt.wavedec(df[channel], wavelet, level=level)\n",
    "\n",
    "# Plot the approximation and detail coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Approximation coefficients (low-frequency components)\n",
    "plt.subplot(level + 2, 1, 1)\n",
    "plt.plot(coeffs[0], label='Approximation')\n",
    "plt.ylabel('Approximation')\n",
    "plt.title('Wavelet Transform')\n",
    "\n",
    "# Detail coefficients (high-frequency components)\n",
    "for i in range(1, level + 1):\n",
    "    plt.subplot(level + 2, 1, i + 1)\n",
    "    plt.plot(coeffs[i], label=f'Detail {i}')\n",
    "    plt.ylabel(f'Detail {i}')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The component with the highest **signal energy** may correspond to the main signal of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:21.801997Z",
     "iopub.status.busy": "2023-06-13T15:43:21.800875Z",
     "iopub.status.idle": "2023-06-13T15:43:22.131917Z",
     "shell.execute_reply": "2023-06-13T15:43:22.131186Z",
     "shell.execute_reply.started": "2023-06-13T15:43:21.801957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute energy of each component\n",
    "energies = [sum(comp ** 2) for comp in coeffs]\n",
    "\n",
    "# Find the index of the component with the highest energy\n",
    "max_energy_index = energies.index(max(energies))\n",
    "print(max_energy_index)\n",
    "\n",
    "# Select the component with the highest energy as the original signal\n",
    "original_signal = coeffs[max_energy_index]\n",
    "\n",
    "\n",
    "\n",
    "#    here I align the x axis due to the downsampling that occurs during the wavelet decomposition process.\n",
    "#    In wavelet decomposition, as you go to higher levels, the signal is typically subsampled by a factor of 2 at each level.\n",
    "#    This downsampling results in a reduced number of samples at each level compared to the original signal.\n",
    "chosen = pywt.upcoef('a', original_signal, wavelet, level=level)   \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "normalized_df.plot(y=channel,label='Original Signal')\n",
    " \n",
    "plt.plot(chosen, label='Upsampled Approximation Coefficients')\n",
    "plt.title('Original Signal vs. Optimal Wavelet Decomp. Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the STD as the metric and form the events the way we want them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DataAnalysis(df_e1, df_f1, 2, 1686188325437, 943743)\n",
    "model2 = DataAnalysis(df_e2, df_f2, 2, 1686188325437, 943743)\n",
    "model3 = DataAnalysis(df_e3, df_f3, 2, 1686488823297, 704365)\n",
    "model4 = DataAnalysis(df_e4, df_f4, 2, 1686490104157, 1985220)\n",
    "model5 = DataAnalysis(df_e5, df_f5, 2, 1686490572615, 2453683)\n",
    "model6 = DataAnalysis(df_e6, df_f6, 2, 1686582948202, 94839527)\n",
    "model7 = DataAnalysis(df_e7, df_f7, 2, 1686583765006, 95656333)\n",
    "model8 = DataAnalysis(df_e8, df_f8, 2, 1686584209841, 96101166)\n",
    "model9 = DataAnalysis(df_e9, df_f9, 2, 1686585053638, 96944963)\n",
    "model10 = DataAnalysis(df_e10, df_f10, 2, 1686585528198, 97419524)\n",
    "model11 = DataAnalysis(df_e11, df_f11, 2, 1686792591196, 32518253)\n",
    "model12 = DataAnalysis(df_e12, df_f12, 2, 1686792984952, 32912013)\n",
    "model13 = DataAnalysis(df_e13, df_f13, 2, 1686793406339, 33333402)\n",
    "model14 = DataAnalysis(df_e14, df_f14, 2, 1686793817357, 33744419)\n",
    "model15 = DataAnalysis(df_e15, df_f15, 2, 1686794381442, 34308500)\n",
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral = []\n",
    "end = []\n",
    "for modeln in models:\n",
    "    neutral.extend(modeln.get_analysis_timestamps()['neutral'])\n",
    "    end.extend(modeln.get_analysis_timestamps()['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:22.391786Z",
     "iopub.status.busy": "2023-06-13T15:43:22.391355Z",
     "iopub.status.idle": "2023-06-13T15:43:22.430335Z",
     "shell.execute_reply": "2023-06-13T15:43:22.429034Z",
     "shell.execute_reply.started": "2023-06-13T15:43:22.391746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_std_columns = []\n",
    "\n",
    "# Iterate over the arrays\n",
    "for arr in neutral:\n",
    "    # Calculate the standard deviation along the columns\n",
    "    std_values = np.std(arr, axis=0)\n",
    "    \n",
    "    # Find the index of the column with the minimum standard deviation\n",
    "    min_std_index = np.argmin(std_values)\n",
    "    \n",
    "    # Extract the column with the minimum standard deviation\n",
    "    min_std_column = arr[:, min_std_index]\n",
    "    \n",
    "    # Append the extracted column to the list\n",
    "    min_std_columns.append(min_std_column)\n",
    "    \n",
    "    \n",
    "#          Normalise them\n",
    "\n",
    "normalized_events = []\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Iterate over the extracted columns\n",
    "for col in min_std_columns:\n",
    "    # Reshape the column to a 2D array (required by MinMaxScaler)\n",
    "    col_2d = col.reshape(-1, 1)\n",
    "\n",
    "    # Normalize the column using MinMaxScaler\n",
    "    normalized_col = scaler.fit_transform(col_2d)\n",
    "    \n",
    "    # Flatten the normalized column back to 1D\n",
    "    normalized_col = normalized_col.flatten()\n",
    "    \n",
    "    # Append the normalized column to the list\n",
    "    normalized_events.append(normalized_col)\n",
    "    \n",
    "train_neutrals= normalized_events\n",
    "\n",
    "print('\\nWe have neutral events: ', len(train_neutrals))\n",
    "\n",
    "\n",
    "\n",
    "#    SAME FOR ENDS\n",
    "\n",
    "min_std_columns = []\n",
    "\n",
    "# Iterate over the arrays\n",
    "for arr in end:\n",
    "    # Calculate the standard deviation along the columns\n",
    "    std_values = np.std(arr, axis=0)\n",
    "    \n",
    "    # Find the index of the column with the minimum standard deviation\n",
    "    min_std_index = np.argmin(std_values)\n",
    "    \n",
    "    # Extract the column with the minimum standard deviation\n",
    "    min_std_column = arr[:, min_std_index]\n",
    "    \n",
    "    # Append the extracted column to the list\n",
    "    min_std_columns.append(min_std_column)\n",
    "      \n",
    "normalized_events = []\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Iterate over the extracted columns\n",
    "for col in min_std_columns:\n",
    "    if(len(col)>0):\n",
    "        # Reshape the column to a 2D array (required by MinMaxScaler)\n",
    "        col_2d = col.reshape(-1, 1)\n",
    "\n",
    "        # Normalize the column using MinMaxScaler\n",
    "        normalized_col = scaler.fit_transform(col_2d)\n",
    "\n",
    "        # Flatten the normalized column back to 1D\n",
    "        normalized_col = normalized_col.flatten()\n",
    "\n",
    "        # Append the normalized column to the list\n",
    "        normalized_events.append(normalized_col)\n",
    "    \n",
    "train_ends= normalized_events\n",
    "\n",
    "print('\\nWe have end events: ', len(train_ends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise one random neutral and one random end state to obseve any possible differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:43:22.432390Z",
     "iopub.status.busy": "2023-06-13T15:43:22.431736Z",
     "iopub.status.idle": "2023-06-13T15:43:23.002926Z",
     "shell.execute_reply": "2023-06-13T15:43:23.001035Z",
     "shell.execute_reply.started": "2023-06-13T15:43:22.432348Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot 2 random states epochs\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_neutrals[37])\n",
    "plt.title('Random Neutral Event')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_ends[37])\n",
    "plt.title('Random End Event')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now see if a machine can see any difference rather than the human eye which is the whole point of Machine Learning's classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:46:41.172684Z",
     "iopub.status.busy": "2023-06-13T15:46:41.172271Z",
     "iopub.status.idle": "2023-06-13T15:46:41.461170Z",
     "shell.execute_reply": "2023-06-13T15:46:41.460141Z",
     "shell.execute_reply.started": "2023-06-13T15:46:41.172652Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# lets put the in a dataframe\n",
    "df_neutrals = pd.DataFrame({'Event': train_neutrals, 'Label': 'neutral'})\n",
    "df_ends = pd.DataFrame({'Event': train_ends, 'Label': 'ends'})\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "selected_samples = pd.concat([df_neutrals, df_ends], ignore_index=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(selected_samples)\n",
    "\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!11\n",
    "# Some events have 1002 points for some reason so we have to make them all same size\n",
    "\n",
    "# Print the size of each event\n",
    "selected_samples['Event_Size'] = selected_samples['Event'].apply(len)\n",
    "print(selected_samples)\n",
    "sample_sizes = selected_samples['Event'].apply(len)\n",
    "distribution = sample_sizes.value_counts().reindex(range(min(sample_sizes), max(sample_sizes) + 1), fill_value=0)\n",
    "\n",
    "# Print the distribution of sample sizes\n",
    "print(\"Distribution of Sample Sizes:\")\n",
    "print(distribution)\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sample_sizes, bins=10)\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sample Sizes')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:49:28.164985Z",
     "iopub.status.busy": "2023-06-13T15:49:28.164572Z",
     "iopub.status.idle": "2023-06-13T15:49:28.207819Z",
     "shell.execute_reply": "2023-06-13T15:49:28.206791Z",
     "shell.execute_reply.started": "2023-06-13T15:49:28.164951Z"
    }
   },
   "outputs": [],
   "source": [
    "###         !!!! Keep ONLY Samples of size 1001\n",
    "\n",
    "filtered_samples = selected_samples[selected_samples['Event'].apply(len) == 1001].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "samples = filtered_samples.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:50:17.733521Z",
     "iopub.status.busy": "2023-06-13T15:50:17.733163Z",
     "iopub.status.idle": "2023-06-13T15:50:17.746860Z",
     "shell.execute_reply": "2023-06-13T15:50:17.745806Z",
     "shell.execute_reply.started": "2023-06-13T15:50:17.733493Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Extract features and labels from the DataFrame\n",
    "X = np.array(samples['Event'].tolist())\n",
    "y = np.array(samples['Label'].tolist())\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the trained model on the test set\n",
    "accuracy = svm_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "\n",
    "The following was created by ChatGPT <3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:59:11.958233Z",
     "iopub.status.busy": "2023-06-13T15:59:11.957836Z",
     "iopub.status.idle": "2023-06-13T15:59:16.515168Z",
     "shell.execute_reply": "2023-06-13T15:59:16.514152Z",
     "shell.execute_reply.started": "2023-06-13T15:59:11.958200Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'MLPClassifier': MLPClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate the classifiers\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Print the results\n",
    "for name, accuracy in results.items():\n",
    "    print(f'{name} Accuracy: {accuracy}')\n",
    "\n",
    "# Plot the confusion matrices\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (name, clf) in enumerate(classifiers.items(), 1):\n",
    "    ax = plt.subplot(2, 4, i)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T15:59:33.389151Z",
     "iopub.status.busy": "2023-06-13T15:59:33.388753Z",
     "iopub.status.idle": "2023-06-13T15:59:35.350209Z",
     "shell.execute_reply": "2023-06-13T15:59:35.349080Z",
     "shell.execute_reply.started": "2023-06-13T15:59:33.389085Z"
    }
   },
   "outputs": [],
   "source": [
    "# classifiers = [\n",
    "#     SVC(),\n",
    "#     RandomForestClassifier(),\n",
    "#     GradientBoostingClassifier(),\n",
    "#     KNeighborsClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     LogisticRegression(),\n",
    "#     MLPClassifier()\n",
    "# ]\n",
    "\n",
    "# # Train and evaluate each classifier\n",
    "# accuracies = []\n",
    "# for classifier in classifiers:\n",
    "#     classifier.fit(X_train, y_train)\n",
    "#     y_pred = classifier.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     accuracies.append(accuracy)\n",
    "\n",
    "# Plotting the accuracies\n",
    "classifiers_names = ['SVM', 'Random Forest', 'Gradient Boosting', 'K-Nearest Neighbors', 'Naive Bayes', 'Logistic Regression', 'MLPClassifier']\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(classifiers.keys(), results[accuracy])\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Classifiers')\n",
    "plt.ylim([0, 1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
